{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20773)\n",
    "\n",
    "pudahuel = pd.read_csv('Pudahuel Data/pudahuel-air-quality.csv', engine='python',\n",
    "                       na_values=[' '])\n",
    "pudahuel.replace('', np.nan, inplace=True)\n",
    "\n",
    "pudahuel[' pm10'] = pudahuel[' pm10'].interpolate()\n",
    "pudahuel[' o3'] = pudahuel[' o3'].interpolate()\n",
    "pudahuel[' o3'] = pudahuel[' o3'] / 1000\n",
    "pudahuel[' so2'] = pudahuel[' so2'].interpolate(method='linear', limit_direction='both')\n",
    "pudahuel['date'] = pd.to_datetime(pudahuel['date']).dt.date\n",
    "pudahuel.sort_values('date', inplace=True, ascending=False)\n",
    "pudahuel.columns = ['date', 'pm25', 'pm10', 'o3', 'no2', 'so2', 'co']\n",
    "\n",
    "print(pudahuel['so2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juntar_datos_csv(nombre_carpeta, arreglo_columnas):\n",
    "    # Obtiene la lista de todos los archivos que coinciden con el patrón\n",
    "    csv_files = glob.glob(nombre_carpeta + \"/*.csv\")\n",
    "    # Crea una lista para almacenar los DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Lee cada archivo CSV y lo agrega a la lista de DataFrames\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file, engine='python', sep=';',\n",
    "                         usecols=arreglo_columnas)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Combina los DataFrames en uno solo\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "\n",
    "# Ejemplo de uso de la función juntar_datos_csv\n",
    "agua_caida = juntar_datos_csv(\"Pudahuel Data/Agua Caida\", [3, 4])\n",
    "presion_humedad = juntar_datos_csv(\"Pudahuel Data/Presion y Humedad\",\n",
    "                                   list(range(3, 10)))\n",
    "radiacion = juntar_datos_csv(\"Pudahuel Data/Radiacion\", [3, 4])\n",
    "viento = juntar_datos_csv(\"Pudahuel Data/Viento\", [3, 5, 6, 7, 8, 9])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formateo de data frames a datos por día"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para datos de Agua Caída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agua_caida)\n",
    "agua_caida['momento'] = pd.to_datetime(agua_caida['momento'])\n",
    "# Agregar una columna \"date\" que contenga solo la fecha (sin la hora)\n",
    "agua_caida['date'] = agua_caida['momento'].dt.date\n",
    "agua_caida_f = agua_caida.groupby('date')['rrInst'].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para datos de Radiación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(radiacion)\n",
    "radiacion['momento'] = pd.to_datetime(radiacion['momento'])\n",
    "# Agregar una columna \"date\" que contenga solo la fecha (sin la hora)\n",
    "radiacion['date'] = radiacion['momento'].dt.date\n",
    "# Obtener los valores máximos para cada día en las columnas especificadas\n",
    "radiacion_f = radiacion.groupby('date')['radiacionGlobalInst'].max()\n",
    "print(radiacion_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para datos del Viento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(viento)\n",
    "viento['momento'] = pd.to_datetime(viento['momento'])\n",
    "# Agregar una columna \"date\" que contenga solo la fecha (sin la hora)\n",
    "viento['date'] = viento['momento'].dt.date\n",
    "# Obtener los valores máximos para cada día en las columnas especificadas\n",
    "#print(viento)\n",
    "#print(viento.groupby('date').max())\n",
    "viento_f = viento.groupby('date')[['ffInst', 'dd02Minutos', 'ff02Minutos',\n",
    "                                   'dd10Minutos', 'ff10Minutos']].max()\n",
    "print(viento_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presion_humedad['momento'] = pd.to_datetime(presion_humedad['momento'])\n",
    "# Agregar una columna \"date\" que contenga solo la fecha (sin la hora)\n",
    "presion_humedad['date'] = presion_humedad['momento'].dt.date\n",
    "# Obtener los valores máximos para cada día en las columnas especificadas\n",
    "presion_humedad_f = presion_humedad.groupby('date')[['hr', 'p0', 'qfe1', 'qfe2', 'qff']]\\\n",
    "                                .mean().reset_index()\n",
    "presion_humedad_f['qfe2'] = presion_humedad_f['qfe2'].interpolate(\n",
    "    method='linear', limit_direction='backward', axis=0)\n",
    "print(presion_humedad_f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construccion Data Frame con todos los datos por fecha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pudahuel.merge(viento_f, on='date')\n",
    "final_df = final_df.merge(presion_humedad_f, on='date')\n",
    "final_df = final_df.merge(agua_caida_f, on='date')\n",
    "final_df = final_df.merge(radiacion_f, on='date')\n",
    "print(final_df)\n",
    "print(final_df.columns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los rangos y ponderaciones utilizados en el cálculo del Índice de Calidad del Aire de Santiago (ICAS) son definidos por la normativa chilena y están diseñados para evaluar la concentración de contaminantes atmosféricos y asignarles un valor numérico representativo de su nivel de riesgo para la salud humana y el medio ambiente.\n",
    "\n",
    "Los rangos representan los límites o umbrales establecidos para cada contaminante. Por ejemplo, para el PM2.5, los rangos indican los intervalos de concentración que van desde niveles bajos hasta niveles peligrosos. Estos rangos pueden variar según el contaminante y la normativa vigente.\n",
    "\n",
    "Las ponderaciones son los valores asignados a cada rango de concentración para determinar la contribución de cada contaminante al índice final. Estas ponderaciones suelen reflejar la importancia relativa de cada contaminante en términos de su impacto en la salud humana y el medio ambiente. Por ejemplo, se pueden asignar ponderaciones más altas a los contaminantes que tienen un mayor impacto en la calidad del aire y en la salud de las personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "# numpy.shape(dataset)\n",
    "print(final_df.columns)\n",
    "\n",
    "\n",
    "# Función para calcular el AQI\n",
    "def calcular_ica_polucion(cp, pollutant):\n",
    "    # Definir los puntos de interrupción y los rangos de AQI\n",
    "    breakpoints = {\n",
    "        'o3': [(0, 0.054), (0.055, 0.070), (0.071, 0.085), (0.086, 0.105),\n",
    "                (0.106, 0.200), (0.201, 0.504)],\n",
    "        'pm25': [(0, 12.0), (12.1, 35.4), (35.5, 55.4), (55.5, 150.4),\n",
    "                 (150.5, 250.4), (250.5, 500.4)],\n",
    "        'pm10': [(0, 54), (55, 154), (155, 254), (255, 354), (355, 424),\n",
    "                 (425, 604)],\n",
    "        'co': [(0, 4.4), (4.5, 9.4), (9.5, 12.4), (12.5, 15.4),\n",
    "               (15.5, 30.4), (30.5, 50.4)],\n",
    "        'so2': [(0, 35), (36, 75), (76, 185), (186, 304), (305, 604),\n",
    "                (605, 1004)],\n",
    "        'no2': [(0, 53), (54, 100), (101, 360), (361, 649),\n",
    "                (650, 1249), (1250, 2049)]\n",
    "    }\n",
    "    rangos_ica = [(0, 50), (51, 100), (101, 150), (151, 200),\n",
    "                  (201, 300), (301, 500)]\n",
    "    for i, (bp_lo, bp_hi) in enumerate(breakpoints[pollutant]):\n",
    "        if bp_lo <= cp <= bp_hi:\n",
    "            i_lo, i_hi = rangos_ica[i]\n",
    "            ip = ((i_hi - i_lo) / (bp_hi - bp_lo)) * (cp - bp_lo) + i_lo\n",
    "            return round(ip)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def calcular_polucion(df):\n",
    "    # Calcular el AQI para cada contaminante y cada fila\n",
    "    for pollutant in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']:\n",
    "        df[f'{pollutant}_ica'] = df[pollutant].apply(\n",
    "            calcular_ica_polucion, pollutant=pollutant)\n",
    "\n",
    "    # Calcular el AQI para cada fila\n",
    "    df['ICA'] = df[[f'{pollutant}_ica'\n",
    "                                for pollutant in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']]].max(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "final_df = calcular_polucion(final_df)\n",
    "\n",
    "# Supongamos que tienes un DataFrame llamado df que contiene los datos de pm25, pm10, o3, no2 y co\n",
    "\n",
    "test = 10\n",
    "\n",
    "print(final_df.query(\"ICA > @test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the dataset\n",
    "# Equation f(yt) = f(yt-5)\n",
    "# Convert an array of values into a dataset matrix\n",
    "\n",
    "# Convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # Input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    # Forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    # Put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    \n",
    "    return agg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para desplazar en un intervalo los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion que crea un dataframe con los valores desplazados en un\n",
    "#  intervalo de tiempo de un data frame entregado\n",
    "def create_interval_shifted_dataset(df, start, end, keep_column):\n",
    "    df = df.sort_values(by='date')\n",
    "    columns = df.columns.tolist()\n",
    "    columns.remove(keep_column)\n",
    "    columns.remove('date')\n",
    "\n",
    "    dfs_auxiliares = {}\n",
    "    for n in range(start, end + 1):\n",
    "        dfs_auxiliares[n] = df.set_index('date').shift(n).reset_index()\n",
    "    new_data = []\n",
    "    for i, row in df.iterrows():\n",
    "        current_date = row['date']\n",
    "        new_row = {}\n",
    "\n",
    "        for n in range(start, end + 1):\n",
    "            df_auxiliar = dfs_auxiliares[n]\n",
    "            df_filtrado = df_auxiliar[df_auxiliar['date'] == current_date]\n",
    "\n",
    "            if not df_filtrado.empty:\n",
    "                for column in columns:\n",
    "                    column_name = f\"{column}(t-{n})\"\n",
    "                    new_row[column_name] = df_filtrado.at[df_filtrado.index[0], column]\n",
    "\n",
    "        if new_row:\n",
    "            new_row[keep_column + \"(t)\"] = row[keep_column]\n",
    "            new_row['date'] = current_date\n",
    "            new_data.append(new_row)\n",
    "\n",
    "    return pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = final_df.copy()\n",
    "print(model_df.columns)\n",
    "model_df.drop(['pm25_ica', 'pm10_ica', 'o3_ica', 'no2_ica', 'so2_ica',\n",
    "               'co_ica', 'rrInst'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(model_df) * 0.8)\n",
    "test_size = len(model_df) - train_size\n",
    "train, test = model_df[0:train_size], model_df[train_size:len(model_df)]\n",
    "print(len(train), len(test))\n",
    "\n",
    "train = create_interval_shifted_dataset(train, 5, 10, 'ICA')\n",
    "train.dropna(inplace=True)\n",
    "train.drop(['date'], axis=1, inplace=True)\n",
    "print(train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.drop(train.columns[list(range(89, 107))], axis=1)\n",
    "#train.columns = ['pm25(t-5)', 'pm10(t-5)', 'o3(t-5)', 'no2(t-5)', 'so2(t-5)',\n",
    "#                 'co(t-5)', 'ffInst(t-5)', 'dd02Minutos(t-5)', 'ff02Minutos(t-5)',\n",
    "#                 'dd10Minutos(t-5)', 'ff10Minutos(t-5)', 'hr(t-5)', 'p0(t-5)',\n",
    "#                 'qfe1(t-5)', 'qfe2(t-5)', 'qff(t-5)', 'rrInst(t-5)',\n",
    "#                 'radiacionGlobalInst(t-5)', 'ICA']\n",
    "\n",
    "# Normalizar los datos\n",
    "scalerX = MinMaxScaler(feature_range=(0, 1))\n",
    "scalerY = MinMaxScaler(feature_range=(0, 1))\n",
    "#train = scalerTrain.fit_transform(train)\n",
    "train = train.values\n",
    "\n",
    "train_x, train_y = train[:, :-1], train[:, -1]\n",
    "\n",
    "train_x = scalerX.fit_transform(train_x)\n",
    "train_y = scalerY.fit_transform(train_y.reshape(-1, 1))\n",
    "\n",
    "#test = test.drop(test.columns[list(range(89, 107))], axis=1)\n",
    "#test.columns = ['pm25(t-5)', 'pm10(t-5)', 'o3(t-5)', 'no2(t-5)', 'so2(t-5)',\n",
    "#               'co(t-5)', 'ffInst(t-5)', 'dd02Minutos(t-5)', 'ff02Minutos(t-5)',\n",
    "#               'dd10Minutos(t-5)', 'ff10Minutos(t-5)', 'hr(t-5)', 'p0(t-5)',\n",
    "#              'qfe1(t-5)', 'qfe2(t-5)', 'qff(t-5)', 'rrInst(t-5)',\n",
    "#             'radiacionGlobalInst(t-5)', 'ICA']\n",
    "\n",
    "test = create_interval_shifted_dataset(test, 5, 10, 'ICA')\n",
    "test.drop(['date'], axis=1, inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "test = test.values\n",
    "test_x, test_y = test[:, :-1], test[:, -1]\n",
    "test_x = scalerX.fit_transform(test_x)\n",
    "test_y = scalerY.fit_transform(test_y.reshape(-1, 1))\n",
    "print(test.shape)\n",
    "\n",
    "#test = test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "test_x = test_x.reshape((test_x.shape[0], train_x.shape[1], 1))\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
    "print(train_x.shape[1], train_x.shape[2])\n",
    "\n",
    "# Graficar train_x y test_x\n",
    "plt.figure()\n",
    "plt.plot(range(train_x.shape[1]), train_x[0, :, 0], 'b-', label='Train Data')\n",
    "plt.plot(range(test_x.shape[1]), test_x[0, :, 0], 'r-', label='Test Data')\n",
    "plt.xlabel('Tiempo')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Datos de entrenamiento y prueba')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "batch_size = 5\n",
    "model = Sequential()\n",
    "model.add(LSTM(20, batch_input_shape=(batch_size, train_x.shape[1], 1), return_sequences=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#pytest\n",
    "\n",
    "history = model.fit(train_x, train_y, epochs=20, batch_size= batch_size, verbose=2, shuffle=False)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predict = model.predict(train_x)\n",
    "print(train_predict.shape)\n",
    "\n",
    "model.reset_states()\n",
    "test_predict = model.predict(test_x)\n",
    "\n",
    "# Graficar train_predict y test_predict\n",
    "plt.figure()\n",
    "plt.plot(range(train_predict.shape[0]), train_predict[:, 0, 0], 'b-', label='Train Predictions')\n",
    "plt.plot(range(test_predict.shape[0]), test_predict[:, 0, 0], 'r-', label='Test Predictions')\n",
    "plt.xlabel('Muestra')\n",
    "plt.ylabel('Valor')\n",
    "plt.xlim([0, 250])\n",
    "plt.title('Predicciones de entrenamiento y prueba')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "# Arreglo shatgipti\n",
    "pred_prueba_y = test_predict.reshape(test_predict.shape[0], test_predict.shape[1])\n",
    "pred_prueba_y = scalerX.inverse_transform(pred_prueba_y)\n",
    "pred_prueba_y = pred_prueba_y[:, 0]\n",
    "\n",
    "test_y = test_y.reshape(len(test_y), 1)\n",
    "test_x2 = test_x.reshape((test_x.shape[0], test_x.shape[1]))\n",
    "data_prueba_y = np.concatenate((test_y, test_x2[:, 1:]), axis=1)\n",
    "data_prueba_y = scalerX.inverse_transform(data_prueba_y)\n",
    "data_prueba_y = data_prueba_y[:, 0]\n",
    "\n",
    "# Graficar los valores reales\n",
    "plt.plot(data_prueba_y, label='Valores reales')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(pred_prueba_y, label='Predicciones')\n",
    "\n",
    "plt.xlabel('df')\n",
    "plt.ylabel('Variable de respuesta')\n",
    "plt.title('Valores reales vs. Predicciones')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_score = math.sqrt(mean_squared_error(data_prueba_y, pred_prueba_y))\n",
    "print('Test Score: %.2f RMSE' % (test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = train_x.reshape((train_x.shape[0], train_x.shape[1]))\n",
    "print(train_X.shape)\n",
    "print(len(train_X[:, 1:]))\n",
    "\n",
    "pred_entrenamiento_y = train_predict.reshape(train_predict.shape[0], train_predict.shape[1])\n",
    "#pred_entrenamiento_y = np.concatenate((pred_entrenamiento_y, train_X[:, 1:]), axis=1)\n",
    "print(pred_entrenamiento_y.shape)\n",
    "\n",
    "pred_entrenamiento_y = scalerX.inverse_transform(pred_entrenamiento_y)\n",
    "pred_entrenamiento_y = pred_entrenamiento_y[:, 0]\n",
    "\n",
    "train_y = train_y.reshape(len(train_y), 1)\n",
    "train_x2 = train_x.reshape((train_x.shape[0], train_x.shape[1]))\n",
    "data_entrenamiento_y = np.concatenate((train_y, train_x2[:, 1:]), axis=1)\n",
    "data_entrenamiento_y = scalerX.inverse_transform(data_entrenamiento_y)\n",
    "data_entrenamiento_y = data_entrenamiento_y[:, 0]\n",
    "\n",
    "# Graficar los valores reales\n",
    "plt.plot(data_entrenamiento_y, label='Valores reales')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(pred_entrenamiento_y, label='Predicciones')\n",
    "\n",
    "plt.xlabel('df')\n",
    "plt.ylabel('Variable de respuesta')\n",
    "plt.title('Valores reales vs. Predicciones')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "train_score = math.sqrt(mean_squared_error(data_entrenamiento_y, pred_entrenamiento_y))\n",
    "print('Train Score: %.2f RMSE' % (train_score))\n",
    "\n",
    "# Graficar los valores reales\n",
    "plt.plot(pred_prueba_y, label='Predicción prueba')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(pred_entrenamiento_y, label='Predicción entrenamiento')\n",
    "\n",
    "# Graficar los valores reales\n",
    "plt.plot(data_prueba_y, label='Valores reales prueba')\n",
    "\n",
    "# Graficar los valores reales entrenamiento\n",
    "plt.plot(data_entrenamiento_y, label='Valores reales entrenamiento')\n",
    "\n",
    "\n",
    "plt.xlabel('df')\n",
    "plt.ylabel('Variable de respuesta')\n",
    "plt.title('prueba vs. entrenamiento vs. reales')\n",
    "plt.legend()\n",
    "\n",
    "# Ajustar el límite del eje x\n",
    "plt.xlim(0, 250)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gestion",
   "language": "python",
   "name": "gestion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
